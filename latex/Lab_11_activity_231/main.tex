\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[rightcaption]{sidecap}
\usepackage{verbatim}
\usepackage[backend=bibtex]{biblatex}
\usepackage [ a4paper , hmargin =1.2 in , bottom =1.5 in ] { geometry }
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{biblatex}


% Add header and footer code here
%\bibliographystyle{plain}
\bibliography{references}

% You may also add path to the images optionally
\graphicspath{{images/}}
\begin{document}
%\begin{center}
\title{Transformation of R.V. \\
and \\
Multivariate Gaussian \\}
\author{somanath vasanth}

\maketitle


% below line auto generates the table of contents
% thank me for your free 1 mark
\tableofcontents
%\clearpage

%code of section 1, with lists
\clearpage
\footnote{$_1$we could have used modulus operator but I wanted things to look more complicated
}
\thispagestyle{fancy}
\lhead{Transformation of R.V. and Multivariate Gaussian}
\rhead{somanathvasanth}
\fancyhf{}
\fancyfoot[C]{Page 2}
\section{Introduction}\label{1}
In this article, we will study about the following topics of statistics:
\begin{itemize}

\item  Transformation of random variables
\item  Multivariate Gaussian random variable
\end{itemize}
\section{Transformation of random variables}
Given any continuous r.v. $X$ with PDF $P_X(x)$ and given any function $g(X)$(defined on range of
$X$) we intend to find PDF associated with the r.v. $Y = g(X)$.
For simplicity, let’s assume $g(.)$ is monotonic increasing.
Then by probability mass conservation,
\[
P(a<X<b)=P(g(a)<Y<g(b))=\int_{g(a)}^{g(b)}Q(y)dy
\]
Using $y = g(x)$, we get the below relation upon simplification
\[
Q(y)=P(g^{-1}(y))\frac{d(g^{-1}(y))}{dy}
\]
To handle monotonically decreasing $g(.)$ as well$^1$,
\begin{equation}
\begin{aligned}
Q(y)=\begin{cases}
+Q(y)=P(g^{-1}(y))\frac{d(g^{-1}(y))}{dy}\\
-Q(y)=P(g^{-1}(y))\frac{d(g^{-1}(y))}{dy}
\end{cases}
\end{aligned}
\end{equation}
For more information,refer\cite{1} 
 \begin{figure}[h!]
 \begin{subfigure}[B]{0.3\textwidth}
\includegraphics[width=\textwidth]{multivariate_gaussian.png}
\caption{Example 1}
\label{fig1}
\end{subfigure}
\begin{subfigure}[B]{0.3\textwidth}
\includegraphics[width=\textwidth]{multivariate_normal.png}
%     % code for subfigure, label them for using references
 \caption{Example 2}
 \label{fig2}
 \end{subfigure}
 \end{figure}
\section{Multi-variate Gaussian Disribution }
\subsection{Defination}
Let $X$ be a vector of random variables of dimension $D$.\\
A r.v. $X$ has a joint PDF as multi-variate Gaussian distribution $\exists$ finite i.i.d. standard Gaussian
\clearpage
\thispagestyle{fancy}
\fancyhf{}
\lhead{Transformation of R.V. and Multivariate Gaussian}
\rhead{somanathvasanth}
\fancyfoot[C]{Page 3}
r.v. $W_1, W_2, . . . W_N $with $N > D$ such that \\
\[X = AW + \mu\] 
Refer fig[\ref{fig1}] and fig[\ref{fig2}] for visual examples. This has many applications in machine learning, refer \cite{2} and \cite{3}.\\

%}

\subsection{ A is diagonal }
In this case, the $X_i$ are independent. The standard deviation of distribution of $X_i$
is $A_{ii}$.

\subsection{A is non-singular square matrix }
Let’s take $\mu = 0$ for simplicity.\\
Similar to univariate case, where scaling was determined by
$|\frac{d(d^{-1}(y))}{dy}|$
, the scaling for multi-variate\\
case is determined by determinant of matrix of derivatives, Jacobian matrix.\\
% Replace '−' with '-' or '--' in equations
Also, $W=A^{-1}X$, which is a linear transformation of vector $X$. $A^{-1}$ maps a hypercube to parallelepiped.
. If the vectors describing the hypercube are along cardinal axis, then the parallelepiped\\
is described by vectors which are columns of $A^{-1}$
.\\%Also, $W=A^{-1}X$, which is a linear transformation of vector $X$. $A^{-1}$ maps a hypercube to parallelepiped.

We intend to find the volume of the parallelepiped formed due to this transformation.\\
\textbf{Claim}: The volume of parallelepiped described by column vectors of matrix $A^{-1}$
is given by\\
$det(A^{-1})$\\
\textbf{Proof}: Addition of any scaled column of a matrix $M$ to another column does not change the\\
determinant.\\
Therefore by Gram-Schmidt orthogonalization process the columns of $A^{-1}$
can be constructed\\
to be orthogonal to each other, without changing the determinant. Then multiplying by an orthogonal matrix would rotate the orthogonal vectors(to align them with cardinal axis), and this\\
operation would not change the determinant as well. Now the result matrix is diagonal square\\
matrix and the volume of the parallelepiped described by the column vectors is given by product\\
of diagonal elements.\\
From the above result, an infinitesimal volume $\delta^D$ after transformation becomes $\delta^D$.$set(A^{-1})$.
Let$C=A \cdot A^T$. Then $det(A) = \sqrt{det(C)}$. The above expression can we rewritten as 

%\begin{equation}

%P(X)\frac{1}{{(2\pi)^{D/2}}} 

%\end{equation}


%code of section 2, make appr
%para
% \[\]
%para.....
\begin{equation}
P(X)=\frac{1}{{2\pi}^{D/2}} \cdot \frac{1}{\sqrt{det(C)}} \cdot exp(0.5 \cdot X^T \cdot C^{-1} \cdot X)
\end{equation}
\begin{center}\begin{tabular}{|c|c||c|}
\hline
\multicolumn{3}{|c|}{Sample Values of bivariate normal distribution}\\
\hline
x&y&f(x,y)\\
\hline
0&0&1.6\\
0&1&0.096\\
$\sqrt{2}$&$\sqrt{2}$&0.02\\
\hline
\end{tabular}
\end{center}




%code for section 3



%
% \begin{tabular}{ ... }
% % ... fill up table
% \end{tabular}

%\begin{bibliography}
%\bibitem{1}
%\end{bibliography}
\printbibliography
% print the bibliography
%\end{center}
\end{document}
